# 리뷰
1. 조직의 의사결정
    - 의사결정유형에 대한 파악이 필요하다는 것을 느낌.
    - 그 전에 우리는 어떻게 의사결정할것인지를 정해야함.

2. RDD
    - RDD로 parquet파일을 읽어보려 했는데, rdd로 바로 읽어들일 수 있는 것은 text파일 밖에 없었음.
    - mapreduce 같이 과거의 것을 보는 느낌이 들었음.
    - 생각해보면 Mapreduce에서 속도를 빠르게 하기 위해 발전한게 Spark.
    - 그렇다면 Spark도 처음에는 라인 정보정도만 읽어서 처리하는 수준이었을 것.
    - 그렇기에 parquet등의 포맷은 지원하지 않았을 것임 (parquet도 없었으려나?)
    - 점점, 행과열로 이루어진 dataframe이라는 개념이 널리 퍼지고 사용되면서, 수요가 생기니 특별히 rdd로 부터 이를 파싱해주는 api가 개발된 것이라고 생각됨.
        - 실제로 1.x에서는 RDD가 주요 인터페이스였지만, 2.x부터는 Dataset API가 등장하면서 Dataset API가 주로 사용되게 되었음. (출처: [위키백과](https://en.wikipedia.org/wiki/Apache_Spark))
    - 만약, parquet파일을 직접 rdd로 읽어서 사용하려면 parquet를 파싱하는 과정이 필요할 듯함. 파싱해도 쓰기 어려울 것 같음.

# 회고
## Problem
1. 팀 활동 진행 과정을 좀 더 기록화하고 정형화 할 수 있게 진행해야한다.
    - 이야기하다보면 어떤 것을 기록해야할지 잘 모르겠다.
    - 의사결정과 그에 대한 팀의 근거를 기록하는 틀을 만들어 놓는 것이 좋을 것 같다.